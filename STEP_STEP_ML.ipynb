{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78daf7b",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "The problem at hand is to predict the housing prices of a town or a suburb based on the features of the locality provided to us. In the process, we need to identify the most important features in the dataset. We need to employ techniques of data preprocessing and build a linear regression model that predicts the prices for us. \n",
    "\n",
    "### Data Information\n",
    "\n",
    "Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. Detailed attribute information can be found below-\n",
    "\n",
    "Attribute Information (in order):\n",
    "- CRIM: per capita crime rate by town( It provides information on the crime rate in different towns or neighborhoods within the Boston area. The value of CRIM indicates the number of crimes per person in a given town.)\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq. ft.\n",
    "- INDUS: proportion of non-retail business acres per town( `Higher INDUS` values suggest a greater proportion of industrial or commercial land, while `lower values` indicate more residential or retail-focused areas.)\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)(Researchers and analysts use this variable to explore how proximity to the river affects housing prices, air quality, and other neighborhood characteristics.)\n",
    "- NX: nitric oxides concentration (parts per 10 million)They are produced during combustion processes, such as those in vehicles, industrial facilities, and power plants.Measurement: The NX value for each town indicates the concentration of nitric oxides in the air. Higher values imply greater pollution levels.\n",
    "- RM: average number of rooms per dwelling\n",
    "- AGE: proportion of owner-occupied units built prior to 1940\n",
    "- DIS: weighted distances to five Boston employment centers(Higher DIS values indicate that a town is farther away from employment centers.(`Higher DIS` values indicate that a town is farther away from employment centers.Lower DIS values suggest closer proximity to employment opportunities.)\n",
    "- RAD: index of accessibility to radial highways\n",
    "- TAX: full-value property-tax rate per 10,000 dollars\n",
    "- PTRATIO: pupil-teacher ratio by town(`Higher PTRATIO` values suggest larger class sizes and potentially less individual attention for students.`Lower PTRATIO` values indicate smaller class sizes and potentially more personalized instruction.)\n",
    "- LSTAT: %lower status of the population\n",
    "- MEDV: Median value of owner-occupied homes in 1000 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12c2ec",
   "metadata": {},
   "source": [
    "## let's load the boston house pricing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461bec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=pd.read_csv('8boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5359ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.MEDV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01622c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2eb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing the stats of the data \n",
    "boston.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value??\n",
    "boston.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value??\n",
    "boston.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb50347",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory data analysis\n",
    "## correlation\n",
    "boston.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9de580",
   "metadata": {},
   "source": [
    "The two types of correlation that we need to check, the correlation between independant features and the correlation between the dependant and the independant features.<br> if there  is high correlation(say 0.9.. or -0.9..) between independant features, you can remove one of the independant feature because that is what is causing what we call multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.pairplot(boston,hue='MEDV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston['CRIM'], boston['MEDV'])\n",
    "plt.xlabel('crime rate')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5230b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston['RM'], boston['MEDV'])\n",
    "plt.xlabel('ROOM')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x='RM',y='MEDV',data=boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06463d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston['LSTAT'], boston['MEDV'])\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6032bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x='LSTAT',y='MEDV',data=boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x='CHAS',y='MEDV',data=boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e112ea",
   "metadata": {},
   "source": [
    "the above features CHAS and MEDV are not correlated and they reduce the error of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x='PTRATIO',y='MEDV',data=boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26cbbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEPENDENT AND DEPENDENT FEATURES\n",
    "X=boston.iloc[:,:-1]\n",
    "y=boston.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceaab992",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.drop([\"MEDV\"], axis=1)\n",
    "y = boston[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d74efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "825d190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train,y_test=train_test_split(X,y, test_size=0.3, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119328e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7d9e0b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standardise the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bd9852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "856f8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i don't want my model to know much about the training data .\n",
    "#that s why i will not do .fit on test data\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93b7ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(scaler,open('scalling.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6000f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37355fe0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e172415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a407f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459148f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the coefficient \n",
    "print (regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ecc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the intercept\n",
    "print(regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on which parameters the model has been trained\n",
    "regression.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f770b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction with test data\n",
    "reg_pred=regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb81df",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "after our assumptions, if we have data point that is very scatter or far away from each other we should know that with a linear regression we will not be able to have a good prediction with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a scatter plot for the prediction\n",
    "plt.scatter(y_test, reg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0b78b",
   "metadata": {},
   "source": [
    "we can see that it is linear that means your model have actually perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8040d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error\n",
    "residuals=y_test -reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the residuals\n",
    "sns.displot(residuals,kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009b863",
   "metadata": {},
   "source": [
    "my residual ranges mostly between -10 to +10; they are some point that are ranging between 10 to 30. we can still assume that our model is performing well because we have a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "##scatter plot with respect to prediction and residuals\n",
    "##uniform distribution\n",
    "plt.scatter(reg_pred, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test,reg_pred))\n",
    "print(mean_squared_error(y_test,reg_pred))\n",
    "print(np.sqrt(mean_squared_error(y_test,reg_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb7e98",
   "metadata": {},
   "source": [
    "# R square and adjusted R square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99976fea",
   "metadata": {},
   "source": [
    "### R^2=1-SSR/SST\n",
    "R^2=coefficient of determination. SSR= sum of squares of residuals. SST = total sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score=r2_score(y_test,reg_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56223df9",
   "metadata": {},
   "source": [
    "the more it is close to 1 the more better our model is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836d780",
   "metadata": {},
   "source": [
    "### Adjusted R2 =1-[(1-R2)*(n-1)/(n-k-1)]\n",
    "where:<br>\n",
    "R2: The R2 of the model; n: The number of observations; k:The number of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818141af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display adjusted R-squared\n",
    "1-(1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a10f2d",
   "metadata": {},
   "source": [
    "# New Data Prediction\n",
    "\n",
    "we want to be able to take a new input data to see what will the prediction be.In most of the data we will be getting single single data point and we have to provide the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ce449",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.iloc[0]. values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.iloc[0]. values.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd1de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ed7ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape your input data\n",
    "X_input = boston.iloc[0].values.reshape(1, -1)\n",
    "\n",
    "# Remove the last feature\n",
    "X_input = X_input[:, :-1]\n",
    "\n",
    "# Make the prediction\n",
    "prediction = regression.predict(X_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b0d280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'training_columns' are the columns used for training the model\n",
    "training_columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "       'PTRATIO', 'LSTAT']  # Replace with actual column names\n",
    "\n",
    "# Align the prediction data with the training columns\n",
    "prediction_data = boston[training_columns].iloc[0].values.reshape(1, -1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = regression.predict(prediction_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ee6b6",
   "metadata": {},
   "source": [
    "we negative value because we did not do standisation on our new data. so below is how to fix it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation of new data.\n",
    "scaler.transform(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.predict(scaler.transform(prediction_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a879b",
   "metadata": {},
   "source": [
    "now we are getting the correct output so what ever step we did from the starting we also have to follow it on the new data .so this is our to do the prediction with respect to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ebd7f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'training_columns' are the columns used for training the model\n",
    "training_columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "       'PTRATIO', 'LSTAT']  # Replace with actual column names\n",
    "\n",
    "# Align the prediction data with the training columns\n",
    "# Make predictions\n",
    "predictions = regression.predict(boston[training_columns].iloc[5].values.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation of new data.\n",
    "scaler.transform(prediction_data)\n",
    "regression.predict(scaler.transform(prediction_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd7d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00d4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7852dae8",
   "metadata": {},
   "source": [
    "# Pickling The Model file For Deployment\n",
    "\n",
    "Now we are going to pickle our model so that we can use it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ddaee29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d0ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert our model into a pickle file.\n",
    "# it is a serialise format so it can be deployed on any server.\n",
    "pickle.dump(regression,open('regmodel.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e77e02",
   "metadata": {},
   "source": [
    "`regression` is the name of the model that i have created<br> `regmodel.pkl` is the file where i will put my model<br>`wb` indicate that if our file does not exit in the folder, it should go ahead and create a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c4d2e",
   "metadata": {},
   "source": [
    "so if you want to see the file that you have created you can go `file, open`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02c83e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this pickle file can also be loaded with the pickle library\n",
    "pickled_model=pickle.load(open('regmodel.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b54b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model.predict(scaler.transform(prediction_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a538802",
   "metadata": {},
   "source": [
    "we can see that we have the same output that we had before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e52d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07881703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "248f7ea9",
   "metadata": {},
   "source": [
    "so far we have created our model for the boston data set and we also tested the new data and finally we pickle the model file for the deployment.<br> now we will see how we can convert this project into an end to end project.<br> when we are creating the end to end project we need to follow the industrial standard; and when we are following the industrial standard we will be using git hub, CICD pipe line we are also use the cloud where we can deploy the application; we will create a simple front end application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f8770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca0044f",
   "metadata": {},
   "source": [
    "Go to git repository web site and download the setup file then install it in your computer.<br>To check if the installation was successful, open command prompt and type `git`. when you see response coming up that means the installation was successful.<br> create a git account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b91020",
   "metadata": {},
   "source": [
    "Go to your git hub profile and open repositories create new repository, enter the name, add the read me file,add in gitignoe the word python because that's what i will be using; pick up any type of licence you want abd you can finally go ahead and create your repository<br> after the repository is created, you have to clone it to your local machine so that you can make commit to it.<br> In other to clone it,  open a command prompt, go to the folder where you which to save your work henceforth type `git clone **the link from your repository**` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c12d3f",
   "metadata": {},
   "source": [
    "So if you go back and open your folder you will see your repository, so the next thing that you need to do is go and copy your pigle file and save it in the same folder as your repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803ee6d",
   "metadata": {},
   "source": [
    "the next thing to do is to go ahead and download visual studio code where we will be doing our entire end to end implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
